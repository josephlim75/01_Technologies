# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# see kafka.server.KafkaConfig for additional details and defaults

############################# Confluent Settings #############################

confluent.support.metrics.enable=false
confluent.support.customer.id=anonymous


# =================================================
# TEDP: Settings
# =================================================

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
zookeeper.connect=leimaprpdn07:2181,leimaprpdn08:2181,leimaprpdn09:2181/kafka

# Zookeeper session timeout.
# Large messages can cause longer garbage collection (GC) pauses as brokers allocate large chunks.
# Monitor the GC log and the server log. If long GC pauses cause Kafka to abandon the ZooKeeper session,
# you may need to configure longer timeout values for zookeeper.session.timeout.ms.
# Default: 6000 (6 secs)
# Custom : 30000 (30 secs)
zookeeper.session.timeout.ms=30000

# The maximum number of unacknowledged requests the client will send to Zookeeper before blocking.
# Default: 10, Custom: 50
zookeeper.max.in.flight.requests=50

# The id of the broker.
# This must be set to a unique integer for each broker.
# Default: -1
broker.id=2

# Enable controlled shutdown of the server
# Default: true
controlled.shutdown.enable=true

# To prevent ungraceful shutdown broker to be chosen as leader when boot up
# Default: true
unclean.leader.election.enable=false

# Enable auto creation of topic on the server
# Default: true
# Custom: false
auto.create.topics.enable=false

# Enables delete topic. Delete topic through the admin tool will have no effect if this config is turned off
# Default: true
delete.topic.enable=true

# Enables auto leader balancing. A background thread checks and triggers leader balance if required at regular intervals.
# Default: true
auto.leader.rebalance.enable=true

# Specify the final compression type for a given topic.
# This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4').
# It additionally accepts 'uncompressed' which is equivalent to no compression; and 'producer'
# which means retain the original compression codec set by the producer.
# Default: producer
compression.type=producer

# Listeners to publish to ZooKeeper for clients to use, if different than the `listeners` config property.
# In IaaS environments, this may need to be different from the interface to which the broker binds.
# If this is not set, the value for `listeners` will be used.
# Unlike `listeners` it is not valid to advertise the 0.0.0.0 meta-address.
# Default: PLAINTEXT://:9092
advertised.listeners=PLAINTEXT://leimaprpdn02:9092

# Listener List - Comma-separated list of URIs we will listen on and the listener names.
# If the listener name is not a security protocol, listener.security.protocol.map must also be set.
# Specify hostname as 0.0.0.0 to bind to all interfaces. Leave hostname empty to bind to default interface.
# Examples of legal listener lists: PLAINTEXT://myhost:9092,SSL://:9091 CLIENT://0.0.0.0:9092,REPLICATION://localhost:9093
# Default: PLAINTEXT://:9092
listeners=PLAINTEXT://:9092

# The directories in which the log data is kept. If not set, the value in log.dir is used
log.dirs=/opt/tsys/edp/confluent/data/logs1,/opt/tsys/edp/confluent/data/logs2,/opt/tsys/edp/confluent/data/logs3,/opt/tsys/edp/confluent/data/logs4,/opt/tsys/edp/confluent/data/logs5,/opt/tsys/edp/confluent/data/logs6,/opt/tsys/edp/confluent/data/logs7,/opt/tsys/edp/confluent/data/logs8,/opt/tsys/edp/confluent/data/logs9,/opt/tsys/edp/confluent/data/logs10,/opt/tsys/edp/confluent/data/logs11,/opt/tsys/edp/confluent/data/logs12

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across the brokers.
# Default: 1
# Custom : 5 (based on the number of brokers)
num.partitions=6

# The number of threads to use for various background processing tasks.
# High volume traffic and more topics requires more background work.
# Default: 10
background.threads=10

# The number of threads that the server uses for processing requests, which may include disk I/O.
# We should have at least as many threads as we have disks.
# Default: 8
# Custom : 8 (We only have 7 disks per node).
num.io.threads=12

# The number of threads that the server uses for receiving requests from the network and sending responses to the network
# Default: 3
# Custom : 12
num.network.threads=12

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
# Default: 1
# Custom : 4 (thread per data directory = 28).
num.recovery.threads.per.data.dir=4

# Number of fetcher threads used to replicate messages from a source broker.
# Increasing this value can increase the degree of I/O parallelism in the follower broker.
# A broker by default uses a single thread to replicate data from another broker,
# so it may take longer to replicate a lot of partitions shared between each pair of brokers and
# consequently take longer for messages to be considered committed.
# Default: 1
# Custom : 8
#        : Large number of topics and partitions require us to increase follower replication.
num.replica.fetchers=8

# If a follower hasn't sent any fetch requests for this window of time, the leader will remove
# the follower from ISR (in-sync replicas) and treat it as dead.
# Default: 10000 (10 seconds)
# Custom : 30000 (30 seconds)
replica.lag.time.max.ms=30000


# The frequency with which the partition rebalance check is triggered by the controller
# Default: 300 (5 mins)
# Custom : 120 (2mins)
leader.imbalance.check.interval.seconds=120

# The number of queued requests allowed before blocking the network threads
# Default: 500
queued.max.requests=500

# The largest record batch size allowed by Kafka.
# If this is increased and there are consumers older than 0.10.2, the consumers'
# fetch size must also be increased so that the they can fetch record batches this large.
# In the latest message format version, records are always grouped into batches for efficiency.
# In previous message format versions, uncompressed records are not grouped into batches and this limit only applies to a single record in that case.
# This can be set per topic with the topic level max.message.bytes config.
# Default: 1000000 (1MB)
# Custom : 2097152 (2 MB)
message.max.bytes=2097152

# Idle connections timeout: the server socket processor threads close the connections that idle more than this.
# Default: 600000 (10 mins)
connections.max.idle.ms=600000

# The SO_RCVBUF buffer of the socket sever sockets. If the value is -1, the OS default will be used.
# /proc/sys/net/core/rmem_max
# Default: 102400 (100 KB)
# Custom : 4194304 (4 MB)
socket.receive.buffer.bytes=-1

# The SO_SNDBUF buffer of the socket sever sockets. If the value is -1, the OS default will be used.
# /proc/sys/net/core/wmem_max
# Default: 102400 (100 KB)
# Custom : 4194304 (4 MB)
socket.send.buffer.bytes=-1

# The maximum size of a request that the socket server will accept (protection against OOM)
# Default: 104857600 (100 MB)
socket.request.max.bytes=104857600

# Specifies the number of bytes of messages to attempt to fetch. This value must be larger than message.max.bytes
# Brokers allocate a buffer the size of replica.fetch.max.bytes for each partition they replicate.
# If replica.fetch.max.bytes is set to 1 MiB, and you have 1000 partitions, about 1 GiB of RAM is required.
# Ensure that the number of partitions multiplied by the size of the largest message does not exceed available memory.
# Default: 1048576 (1 MB)
# Custom : 2097152 (2 MB) - With 16 Topics at 360 Partitions will require about 12GiB of RAM.
replica.fetch.max.bytes=2097152

# When a producer sets acks to "all" (or "-1"), min.insync.replicas specifies
# the minimum number of replicas that must acknowledge a write for the write to be considered successful.
# If this minimum cannot be met, then the producer will raise an exception (either NotEnoughReplicas or NotEnoughReplicasAfterAppend).
# When used together, min.insync.replicas and acks allow you to enforce greater durability guarantees.
# A typical scenario would be to create a topic with a replication factor of 3, set min.insync.replicas to 2, and produce with acks of "all".
# This will ensure that the producer raises an exception if a majority of replicas do not receive a write.
# Default: 1, Custom: 2
min.insync.replicas=2

# The amount of time the group coordinator will wait for more consumers to join a new group before performing the first rebalance.
# A longer delay means potentially fewer rebalances, but increases the time until processing begins.
# Default: 3000
group.initial.rebalance.delay.ms=3000

# https://cwiki.apache.org/confluence/display/KAFKA/KIP-62%3A+Allow+consumer+to+send+heartbeats+from+a+background+thread
# The maximum delay between invocations of poll() when using consumer group management.
# This places an upper bound on the amount of time that the consumer can be idle before fetching more records.
# If poll() is not called before expiration of this timeout, then the consumer is considered failed and the group
# will rebalance in order to reassign the partitions to another member.
# Default: 300000 (5 mins)
# Custom : 120000 (2 mins)
max.poll.interval.ms=120000

# The maximum number of records returned in a single call to poll().
# Default: 500
max.poll.records=500

# Offsets older than this retention period will be discarded
# Default: 10080 (7 days)
# Custom : 129600 (90 days)
offsets.retention.minutes=129600

# The number of partitions for the offset commit topic (should not change after deployment)
# Default: 50, Custom: 180
offsets.topic.num.partitions=50

# The replication factor for the offsets topic (set higher to ensure availability).
# Internal topic creation will fail until the cluster size meets this replication factor requirement.
# Default: 3
offsets.topic.replication.factor=3

# Define whether the timestamp in the message is message create time or log append time. The value should be either "CreateTime" or "LogAppendTime"
# [Event-time] processing semantics if log.message.timestamp.type is set to CreateTime aka "producer time" (which is the default). This represents the time when the Kafka producer sent the original message.
# [Ingestion-time] processing semantics if log.message.timestamp.type is set to LogAppendTimeaka "broker time". This represents the time when the Kafka broker received the original message.
# Default: CreateTime
log.message.timestamp.type=LogAppendTime

# The replication factor for the transaction topic (set higher to ensure availability).
# Internal topic creation will fail until the cluster size meets this replication factor requirement.
# Default: 3, Custom: 2
transaction.state.log.replication.factor=2

# Overridden min.insync.replicas config for the transaction topic.
# Default: 2
transaction.state.log.min.isr=2

# Offset commit will be delayed until all replicas for the offsets topic receive the commit or this timeout is reached.
# This is similar to the producer request timeout.
# Default: 5000 (5 secs)
# Custom : 20000 (20 secs)
offsets.commit.timeout.ms=20000

# Batch size for reading from the offsets segments when loading offsets into the cache.
# offsets.load.buffer.size=5242880

# Frequency at which to check for stale offsets
# offsets.retention.check.interval.ms=600000

# Messages are immediately written to the filesystem but by default we only fsync() to sync the OS cache lazily.
# The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.
# The number of messages accumulated on a log partition before messages are flushed to disk.
# Default: 9223372036854775807
# Custom : 1000000
log.flush.interval.messages=9223372036854775807

# The frequency in ms that the log flusher checks whether any log needs to be flushed to disk.
# Default: 9223372036854775807 (Many years)
# Custom : 1800000 (30 mins)
log.flush.scheduler.interval.ms=9223372036854775807

# The frequency with which we update the persistent record of the last flush which acts as the log recovery point
# Default: 60000 (1 min)
log.flush.offset.checkpoint.interval.ms=60000

# The number of hours to keep a log file before deleting it (in hours), tertiary to log.retention.ms property
# Deletion always happens from the end of the log.
# Default: 168 (7 days)
log.retention.hours=168

# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
# Default: -1
log.retention.bytes=-1

# The interval at which log segments are checked to see if they can be deleted according to the retention policies
# Default: 300000 (5 mins)
#log.retention.check.interval.ms=300000

# The maximum time before a new log segment is rolled out (in hours)
# This setting controls the period of time after which Kafka will force the log to roll, even if the segment file is not full.
# This ensures that the retention process is able to delete or compact old data.
# Default: 168 (7 days)
log.roll.hours=168

# The maximum size of a log segment file.
# When this size is reached a new log segment will be created.
# Default: 1073741824 (1GB)
#          Increasing the segment file size reducing the number of files in disk and the number of "open file handles".
#          Segment files will be kept around for months because we have longer retention periods, therefore larger files is better.
#          Bigger files may impact time to delete and time to allocate new segment files.
log.segment.bytes=1073741824

# The amount of time to wait before deleting a segment file from the filesystem
# Default: 60000 (1 min)
# Custom : 300000 (5 mins)
#          We don't need the deletion to occur so frequently because we have the disk space available.
#          Increasing this hopefully will reduce the amount spent looking for log files to delete.
log.segment.delete.delay.ms=300000
